import os
import sqlite3
import time
from datetime import datetime, date
from typing import Any, Dict, List, Optional

DB_PATH = os.path.join("data", "project_puma.db")

def _now_iso() -> str:
    return datetime.now().isoformat(timespec="seconds")

def _try_get_sf_session():
    try:
        from snowflake.snowpark.context import get_active_session  # type: ignore
        return get_active_session()
    except Exception:
        return None

def backend() -> str:
    return "snowflake" if _try_get_sf_session() is not None else "sqlite"

def _retry_locked(fn, retries: int = 20):
    for i in range(retries):
        try:
            return fn()
        except sqlite3.OperationalError as e:
            if "locked" in str(e).lower():
                time.sleep(0.15 * (i + 1))
                continue
            raise
    raise sqlite3.OperationalError("database is locked (retry exhausted)")

# ---------------- SQLITE ----------------
def _sqlite_conn():
    os.makedirs(os.path.dirname(DB_PATH), exist_ok=True)
    # isolation_level=None = autocommit (reduces weird lock behavior around PRAGMAs)
    conn = sqlite3.connect(DB_PATH, timeout=30, isolation_level=None, check_same_thread=False)
    conn.row_factory = sqlite3.Row

    # Always set busy timeout first
    conn.execute("PRAGMA busy_timeout=30000;")
    conn.execute("PRAGMA foreign_keys=ON;")

    # WAL is great, but it can be locked briefly if another process has the DB open.
    # So we retry instead of crashing the app.
    for i in range(30):
        try:
            conn.execute("PRAGMA journal_mode=WAL;")
            conn.execute("PRAGMA synchronous=NORMAL;")
            break
        except sqlite3.OperationalError as e:
            if "locked" in str(e).lower():
                time.sleep(0.15 * (i + 1))
                continue
            raise

    return conn

def _sqlite_cols(conn: sqlite3.Connection, table: str) -> List[str]:
    rows = conn.execute(f"PRAGMA table_info({table});").fetchall()
    return [r["name"] for r in rows]

def _sqlite_shift_user(conn: sqlite3.Connection, shift_id: int) -> str:
    cols = set(_sqlite_cols(conn, "shifts"))
    user_col = "username" if "username" in cols else ("active_user" if "active_user" in cols else None)
    if user_col is None:
        return ""
    row = conn.execute(f"SELECT {user_col} FROM shifts WHERE id=?", (int(shift_id),)).fetchone()
    if not row:
        return ""
    return row[0] if row[0] else ""

def _sqlite_activity_schema(conn: sqlite3.Connection) -> str:
    cols = set(_sqlite_cols(conn, "activities"))
    if "start_ts" in cols:
        return "new"
    if "start_time" in cols:
        return "legacy"
    raise sqlite3.OperationalError("activities table schema not recognized")

def init_storage() -> None:
    if backend() == "snowflake":
        s = _try_get_sf_session()
        assert s is not None
        # Put tables in the app schema.
        s.sql("""
        CREATE TABLE IF NOT EXISTS PUMA_SHIFTS(
          ID NUMBER GENERATED BY DEFAULT AS IDENTITY,
          SHIFT_DATE DATE NOT NULL,
          SHIFT_TYPE STRING NOT NULL,
          USERNAME STRING NOT NULL,
          VEHICLE STRING NOT NULL,
          JOB_NUMBER STRING,
          SITE_NAME STRING,
          SHIFT_START STRING,
          SHIFT_HOURS FLOAT,
          SHIFT_NOTES STRING,
          CREATED_AT TIMESTAMP_NTZ NOT NULL DEFAULT CURRENT_TIMESTAMP(),
          UPDATED_AT TIMESTAMP_NTZ NOT NULL DEFAULT CURRENT_TIMESTAMP()
        );
        """).collect()

        s.sql("""
        CREATE TABLE IF NOT EXISTS PUMA_ACTIVITIES(
          ID NUMBER GENERATED BY DEFAULT AS IDENTITY,
          SHIFT_ID NUMBER NOT NULL,
          START_TS TIMESTAMP_NTZ NOT NULL,
          END_TS TIMESTAMP_NTZ,
          CODE STRING NOT NULL,
          TITLE STRING NOT NULL,
          NOTES STRING,
          TOOL_REF STRING,
          CREATED_AT TIMESTAMP_NTZ NOT NULL DEFAULT CURRENT_TIMESTAMP(),
          UPDATED_AT TIMESTAMP_NTZ NOT NULL DEFAULT CURRENT_TIMESTAMP()
        );
        """).collect()
        return

    conn = _sqlite_conn()
    cur = conn.cursor()

    cur.execute("""
    CREATE TABLE IF NOT EXISTS shifts(
      id INTEGER PRIMARY KEY AUTOINCREMENT,
      shift_date TEXT NOT NULL,
      shift_type TEXT NOT NULL,
      username TEXT NOT NULL,
      vehicle TEXT NOT NULL,
      job_number TEXT,
      site_name TEXT,
      shift_start TEXT,
      shift_hours REAL,
      shift_notes TEXT,
      created_at TEXT NOT NULL,
      updated_at TEXT NOT NULL,
      UNIQUE(shift_date, shift_type, username)
    );
    """)

    cur.execute("""
    CREATE TABLE IF NOT EXISTS activities(
      id INTEGER PRIMARY KEY AUTOINCREMENT,
      shift_id INTEGER NOT NULL,
      start_ts TEXT NOT NULL,
      end_ts TEXT,
      code TEXT NOT NULL,
      title TEXT NOT NULL,
      notes TEXT,
      tool_ref TEXT,
      created_at TEXT NOT NULL,
      updated_at TEXT NOT NULL,
      FOREIGN KEY(shift_id) REFERENCES shifts(id)
    );
    """)

    conn.commit()

    # Migrate older DBs (add any missing columns)
    cols = _sqlite_cols(conn, "shifts")
    def add_col(name: str, ddl: str):
        if name not in cols:
            _retry_locked(lambda: cur.execute(ddl))

    add_col("job_number", "ALTER TABLE shifts ADD COLUMN job_number TEXT;")
    add_col("site_name", "ALTER TABLE shifts ADD COLUMN site_name TEXT;")
    add_col("shift_start", "ALTER TABLE shifts ADD COLUMN shift_start TEXT;")
    add_col("shift_hours", "ALTER TABLE shifts ADD COLUMN shift_hours REAL;")
    add_col("shift_notes", "ALTER TABLE shifts ADD COLUMN shift_notes TEXT;")

    conn.commit()
    conn.close()

def get_shift(shift_date: date, shift_type: str, username: str) -> Optional[Dict[str, Any]]:
    init_storage()
    if backend() == "snowflake":
        s = _try_get_sf_session()
        assert s is not None
        rows = s.sql("""
          SELECT *
          FROM PUMA_SHIFTS
          WHERE SHIFT_DATE = ? AND SHIFT_TYPE = ? AND USERNAME = ?
          ORDER BY UPDATED_AT DESC, ID DESC
          LIMIT 1
        """, params=[shift_date, shift_type, username]).collect()
        if not rows:
            return None
        r = rows[0]
        return {
            "id": int(r["ID"]),
            "shift_date": str(r["SHIFT_DATE"]),
            "shift_type": r["SHIFT_TYPE"],
            "username": r["USERNAME"],
            "vehicle": r["VEHICLE"],
            "job_number": r.get("JOB_NUMBER"),
            "site_name": r.get("SITE_NAME"),
            "shift_start": r.get("SHIFT_START"),
            "shift_hours": r.get("SHIFT_HOURS"),
            "shift_notes": r.get("SHIFT_NOTES"),
        }

    conn = _sqlite_conn()
    cols = set(_sqlite_cols(conn, "shifts"))

    row = None
    if "username" in cols:
        row = conn.execute(
            "SELECT * FROM shifts WHERE shift_date=? AND shift_type=? AND username=?",
            (shift_date.isoformat(), shift_type, username),
        ).fetchone()

    if row is None and "active_user" in cols:
        row = conn.execute(
            "SELECT * FROM shifts WHERE shift_date=? AND shift_type=? AND active_user=?",
            (shift_date.isoformat(), shift_type, username),
        ).fetchone()

    if row is None and "username" in cols:
        row = conn.execute(
            "SELECT * FROM shifts WHERE shift_date=? AND shift_type=? ORDER BY updated_at DESC, id DESC LIMIT 1",
            (shift_date.isoformat(), shift_type),
        ).fetchone()

    conn.close()
    if not row:
        return None

    result = dict(row)
    normalized_username = result.get("active_user") or result.get("username") or username
    result["username"] = normalized_username
    return result

def upsert_shift(
    shift_date: date,
    shift_type: str,
    username: str,
    vehicle: str,
    job_number: str = "",
    site_name: str = "",
    shift_start: str = "",
    shift_hours: float = 12.0,
    shift_notes: str = "",
) -> int:
    init_storage()
    if backend() == "snowflake":
        s = _try_get_sf_session()
        assert s is not None

        existing = get_shift(shift_date, shift_type, username)
        if existing:
            sid = int(existing["id"])
            s.sql("""
              UPDATE PUMA_SHIFTS
              SET VEHICLE=?, JOB_NUMBER=?, SITE_NAME=?, SHIFT_START=?, SHIFT_HOURS=?, SHIFT_NOTES=?, UPDATED_AT=CURRENT_TIMESTAMP()
              WHERE ID=?
            """, params=[vehicle, job_number, site_name, shift_start, float(shift_hours), shift_notes, sid]).collect()
            return sid

        s.sql("""
          INSERT INTO PUMA_SHIFTS(SHIFT_DATE, SHIFT_TYPE, USERNAME, VEHICLE, JOB_NUMBER, SITE_NAME, SHIFT_START, SHIFT_HOURS, SHIFT_NOTES)
          VALUES(?,?,?,?,?,?,?,?,?)
        """, params=[shift_date, shift_type, username, vehicle, job_number, site_name, shift_start, float(shift_hours), shift_notes]).collect()

        created = get_shift(shift_date, shift_type, username)
        assert created is not None
        return int(created["id"])

    conn = _sqlite_conn()
    cur = conn.cursor()
    cols = set(_sqlite_cols(conn, "shifts"))
    now = _now_iso()

    existing = None
    if "username" in cols:
        existing = conn.execute(
            "SELECT id FROM shifts WHERE shift_date=? AND shift_type=? AND username=?",
            (shift_date.isoformat(), shift_type, username),
        ).fetchone()
    if existing is None and "active_user" in cols:
        existing = conn.execute(
            "SELECT id FROM shifts WHERE shift_date=? AND shift_type=? AND active_user=?",
            (shift_date.isoformat(), shift_type, username),
        ).fetchone()

    # Always avoid NULLs for legacy NOT NULL columns.
    job_number_val = job_number or ""
    site_name_val = site_name or None
    shift_start_val = shift_start or ""
    shift_hours_val = float(shift_hours)
    shift_notes_val = shift_notes or None
    active_user_val = username
    synced_val = 0

    if existing:
        sid = int(existing["id"])
        set_cols = []
        set_vals = []
        def setcol(name: str, val):
            if name in cols:
                set_cols.append(f"{name}=?")
                set_vals.append(val)

        setcol("username", username)
        setcol("vehicle", vehicle)
        setcol("job_number", job_number_val)
        setcol("site_name", site_name_val)
        setcol("shift_start", shift_start_val)
        setcol("shift_hours", shift_hours_val)
        setcol("shift_notes", shift_notes_val)
        setcol("active_user", active_user_val)
        setcol("synced", synced_val)
        setcol("updated_at", now)

        if set_cols:
            set_vals.append(sid)
            _retry_locked(lambda: cur.execute(
                f"UPDATE shifts SET {', '.join(set_cols)} WHERE id=?",
                tuple(set_vals),
            ))
    else:
        insert_cols = []
        insert_vals = []
        def add(name: str, val):
            if name in cols:
                insert_cols.append(name)
                insert_vals.append(val)

        add("shift_date", shift_date.isoformat())
        add("shift_type", shift_type)
        add("username", username)
        add("vehicle", vehicle)
        add("job_number", job_number_val)
        add("site_name", site_name_val)
        add("shift_start", shift_start_val)
        add("shift_hours", shift_hours_val)
        add("shift_notes", shift_notes_val)
        add("active_user", active_user_val)
        add("synced", synced_val)
        add("created_at", now)
        add("updated_at", now)

        placeholders = ",".join(["?"] * len(insert_cols))
        _retry_locked(lambda: cur.execute(
            f"INSERT INTO shifts({', '.join(insert_cols)}) VALUES({placeholders})",
            tuple(insert_vals),
        ))
        sid = int(cur.lastrowid)

    conn.commit()
    conn.close()
    return int(sid)

def list_activities(shift_id: int) -> List[Dict[str, Any]]:
    init_storage()
    if backend() == "snowflake":
        s = _try_get_sf_session()
        assert s is not None
        rows = s.sql("""
          SELECT
            ID, SHIFT_ID,
            TO_VARCHAR(START_TS, 'YYYY-MM-DD\"T\"HH24:MI:SS') AS START_TS,
            IFF(END_TS IS NULL, NULL, TO_VARCHAR(END_TS, 'YYYY-MM-DD\"T\"HH24:MI:SS')) AS END_TS,
            CODE, TITLE, NOTES, TOOL_REF
          FROM PUMA_ACTIVITIES
          WHERE SHIFT_ID = ?
          ORDER BY START_TS ASC, ID ASC
        """, params=[int(shift_id)]).collect()
        return [r.as_dict() for r in rows]

    conn = _sqlite_conn()
    schema = _sqlite_activity_schema(conn)
    rows: List[sqlite3.Row]
    if schema == "new":
        rows = conn.execute(
            "SELECT * FROM activities WHERE shift_id=? ORDER BY start_ts ASC, id ASC",
            (int(shift_id),),
        ).fetchall()
        result = [dict(r) for r in rows]
    else:
        rows = conn.execute(
            "SELECT * FROM activities WHERE shift_id=? ORDER BY start_time ASC, id ASC",
            (int(shift_id),),
        ).fetchall()
        result = []
        for r in rows:
            d = dict(r)
            result.append({
                "id": d.get("id"),
                "shift_id": d.get("shift_id"),
                "start_ts": d.get("start_ts") or d.get("start_time"),
                "end_ts": d.get("end_ts") or d.get("end_time"),
                "code": d.get("code"),
                "title": d.get("title") or d.get("description"),
                "notes": d.get("notes") or d.get("comments") or "",
                "tool_ref": d.get("tool_ref") or d.get("tools_csv") or "",
            })

    conn.close()
    return result

def add_activity(shift_id: int, start_ts: str, end_ts: Optional[str], code: str, title: str, notes: str = "", tool_ref: str = "") -> None:
    init_storage()
    if backend() == "snowflake":
        s = _try_get_sf_session()
        assert s is not None
        s.sql("""
          INSERT INTO PUMA_ACTIVITIES(SHIFT_ID, START_TS, END_TS, CODE, TITLE, NOTES, TOOL_REF)
          SELECT
            ?, TO_TIMESTAMP_NTZ(?),
            IFF(? IS NULL OR ? = '', NULL, TO_TIMESTAMP_NTZ(?)),
            ?, ?, NULLIF(?, ''), NULLIF(?, '')
        """, params=[int(shift_id), start_ts, end_ts, end_ts or "", end_ts or "", code, title, notes or "", tool_ref or ""]).collect()
        return

    conn = _sqlite_conn()
    cur = conn.cursor()
    schema = _sqlite_activity_schema(conn)
    now = _now_iso()

    if schema == "new":
        _retry_locked(lambda: cur.execute("""
          INSERT INTO activities(shift_id, start_ts, end_ts, code, title, notes, tool_ref, created_at, updated_at)
          VALUES(?,?,?,?,?,?,?,?,?)
        """, (int(shift_id), start_ts, end_ts, code, title, notes or None, tool_ref or None, now, now)))
    else:
        activity_user = _sqlite_shift_user(conn, shift_id)
        end_value = end_ts or start_ts  # legacy schema requires a non-NULL end time
        _retry_locked(lambda: cur.execute("""
          INSERT INTO activities(shift_id, start_time, end_time, code, description, tools_csv, comments, qaqc, user_name, created_at, updated_at)
          VALUES(?,?,?,?,?,?,?,?,?,?,?)
        """, (
            int(shift_id), start_ts, end_value, code, title, tool_ref or None, notes or None, None,
            activity_user or "", now, now,
        )))

    conn.commit()
    conn.close()

def delete_activity(activity_id: int) -> None:
    init_storage()
    if backend() == "snowflake":
        s = _try_get_sf_session()
        assert s is not None
        s.sql("DELETE FROM PUMA_ACTIVITIES WHERE ID=?", params=[int(activity_id)]).collect()
        return

    conn = _sqlite_conn()
    _retry_locked(lambda: conn.execute("DELETE FROM activities WHERE id=?", (int(activity_id),)))
    conn.commit()
    conn.close()
